# -*- coding: utf-8 -*-
"""Plant Disease Detection - ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hi-IpYhHkFb4XK4c36rFZiScA1yOt3xt

# **Plant Disease Detection using Vision Transformer**

This project uses a Vision Transformer (ViT) to detect and classify plant diseases from leaf images. The dataset includes 15 classes of tomato, pepper, and potato plant leaves, both healthy and diseased. This model helps farmers and agricultural professionals diagnose plant diseases early using AI.

## **Install Required Libraries**
"""

# Install timm for pretrained ViT models
!pip install timm torch torchvision split-folders
!pip install kaggle

# Import required libraries
import torch
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import timm  # Library for pretrained models
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import splitfolders  # For splitting dataset into train/test
import os
from google.colab import files

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Step 3: Download and Prepare Dataset
# Check if dataset is already downloaded

if not os.path.exists("PlantVillage"):
    files.upload()  # Upload kaggle.json API key
    !mkdir ~/.kaggle
    !mv kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    !kaggle datasets download -d emmarex/plantdisease
    !unzip plantdisease.zip -d PlantVillage

# Split dataset into train (80%) and test (20%) sets
splitfolders.ratio("PlantVillage/PlantVillage", output="PlantVillage_Split",
                   seed=42, ratio=(0.8, 0.2), group_prefix=None)

# Define dataset path after splitting
dataset_path = "PlantVillage_Split"

"""## **Transform Images**"""

# Data augmentation to help model generalize better
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to 224x224 (ViT input size)
    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally
    transforms.RandomRotation(30),  # Rotate images randomly (up to 30 degrees)
    transforms.ColorJitter(brightness=0.3, contrast=0.3),  # Adjust brightness & contrast
    transforms.ToTensor(),  # Convert image to PyTorch tensor
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize using ImageNet stats
])

# Load dataset (train & test)
train_dataset = datasets.ImageFolder(root=f"{dataset_path}/train", transform=transform)
test_dataset = datasets.ImageFolder(root=f"{dataset_path}/val", transform=transform)

# Define DataLoaders for batch processing
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Print dataset info
print(f"Number of Classes: {len(train_dataset.classes)}")
print(f"Class Labels: {train_dataset.classes}")

"""## **Load Pretrained Vision Transformer**"""

# Load ViT model pretrained on ImageNet, modify output for our dataset
model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(train_dataset.classes))
model = model.to(device)  # Move model to GPU if available

# Freeze base layers (only fine-tune classification head)
for param in model.parameters():
    param.requires_grad = False  # Keep pretrained layers unchanged

# Unfreeze classification head (fine-tune only this part)
for param in model.head.parameters():
    param.requires_grad = True

"""## **Fine-tuning: Define Loss Function & Optimizer**"""

# Step 6: Define Loss Function & Optimizer

# Cross-Entropy Loss for multi-class classification
criterion = nn.CrossEntropyLoss()

# AdamW optimizer (better weight regularization)
optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)

# Learning rate scheduler to improve training performance
scheduler = CosineAnnealingLR(optimizer, T_max=10)

"""## **Train The Model**"""

# Step 7: Define Training Function
def train_model(model, train_loader, epochs=10):
    model.train()  # Set model to training mode

    for epoch in range(epochs):
        if epoch == 5:  # Unfreeze all layers after 5 epochs
            for param in model.parameters():
                param.requires_grad = True

        running_loss = 0.0  # Track loss per epoch
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU

            optimizer.zero_grad()  # Clear previous gradients
            outputs = model(inputs)  # Forward pass
            loss = criterion(outputs, labels)  # Compute loss
            loss.backward()  # Backpropagation
            optimizer.step()  # Update weights

            running_loss += loss.item()  # Accumulate loss

        scheduler.step()  # Adjust learning rate
        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}")

"""## **Evaluate The Model**"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix
# Step 8: Define Evaluation Function
def evaluate_model(model, test_loader, class_names):
    """Evaluate model accuracy and display confusion matrix."""
    model.eval()  # Set model to evaluation mode
    correct, total = 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate Accuracy
    accuracy = 100 * correct / total
    print(f"Test Accuracy: {accuracy:.2f}%")

    # Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

    return accuracy

"""## **Results**"""

train_model(model, train_loader, epochs=10)  # Train for 10 epochs

evaluate_model(model, test_loader, train_dataset.classes)  # Evaluate model performance